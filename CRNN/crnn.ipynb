{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "GISa6gFL88VN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1jdLrbwesgz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount( '/content/gdrive' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "2faKA0Jk8v1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision tqdm\n",
        "!pip install ultralytics\n",
        "!pip install python-Levenshtein\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "metadata": {
        "id": "I69t-tAlesg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "acZsx2-89G7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision import models\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import Image as IPImage, display\n",
        "import glob # For finding files\n",
        "import re # For plausible plate filtering\n",
        "from collections import Counter # For majority vote\n",
        "import tensorflow as tf\n",
        "from Levenshtein import distance as levenshtein_distance # For CER\n",
        "\n",
        "# --- FORCE TENSORFLOW TO CPU (FOR DEBUGGING EFFICIENTDET) ---\n",
        "# This MUST be one of the first TF operations\n",
        "print(\"Attempting to force TensorFlow operations to CPU for debugging EfficientDet...\")\n",
        "try:\n",
        "    # Get a list of physical GPUs\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        # If there are GPUs, prevent TensorFlow from using them\n",
        "        tf.config.set_visible_devices([], 'GPU')\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"Physical GPUs: {len(gpus)}, Logical GPUs after hiding: {len(logical_gpus)}\")\n",
        "        print(\"‚úÖ GPUs should now be hidden from TensorFlow. Operations will attempt CPU.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPUs found. TensorFlow will use CPU.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"‚ö†Ô∏è Could not set visible devices (might have already been set or other issue): {e}\")\n",
        "# --- END OF CPU FORCING BLOCK ---\n",
        "\n",
        "print(\"‚úÖ Libraries imported\")"
      ],
      "metadata": {
        "id": "jvS-dlINesg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path Setup"
      ],
      "metadata": {
        "id": "en8SMSpy9M1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/gdrive/MyDrive/cos30018-test/CRNN_CTC_Loss\"\n",
        "training_annotations_xml_path = '/content/gdrive/MyDrive/cos30018-test/data/annotations_main.xml'\n",
        "validation_annotations_xml_path = '/content/gdrive/MyDrive/cos30018-test/data/annotations_val.xml'\n",
        "training_image_path = '/content/gdrive/MyDrive/cos30018-test/data/images/train'\n",
        "validation_image_path = '/content/gdrive/MyDrive/cos30018-test/data/images/val'\n",
        "cropped_image_path = os.path.join(output_path, \"cropped_images\")\n",
        "cropped_image_val_path = os.path.join(output_path, \"cropped_images_val\")\n",
        "training_labels_csv_path = os.path.join(output_path, \"labels.csv\")\n",
        "validation_labels_csv_path = os.path.join(output_path, \"labels_val.csv\")\n",
        "best_model_path = os.path.join(output_path, \"crnn_best_model.pth\")\n",
        "predictions_csv_path = os.path.join(output_path, \"validation_predictions.csv\")\n",
        "yolov11_model_path = \"/content/gdrive/MyDrive/cos30018-test/yolov11/train_170525/runs/detect/train2/weights/best.pt\"\n",
        "yolov10_model_path = \"/content/gdrive/MyDrive/cos30018-test/PaddleOCR/PaddleOCR/weights/best.pt\"\n",
        "efficientdet_model_path = \"/content/gdrive/MyDrive/cos30018-test/efficientdetd0/saved_model/\"\n",
        "annotated_output_path = os.path.join(output_path, \"od_crnn_predictions\")\n",
        "\n",
        "# Check if paths exist\n",
        "paths_name = [\"training_annotations_xml_path\", \"validation_annotations_xml_path\", \"training_image_path\", \"validation_image_path\", \"cropped_image_path\", \"cropped_image_val_path\", \"training_labels_csv_path\", \"validation_labels_csv_path\", \"best_model_path\", \"predictions_csv_path\", \"yolov11_model_path\", \"yolov10_model_path\", \"efficientdet_model_path\", \"annotated_output_path\"]\n",
        "paths_to_check = [training_annotations_xml_path, validation_annotations_xml_path, training_image_path, validation_image_path, cropped_image_path, cropped_image_val_path, training_labels_csv_path, validation_labels_csv_path, best_model_path, predictions_csv_path, yolov11_model_path, yolov10_model_path, efficientdet_model_path, annotated_output_path]\n",
        "print(\"\\nChecking paths...\")\n",
        "for path_name, path in zip(paths_name, paths_to_check):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úÖ {path_name} exists: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {path_name} does not exist: {path}\")\n",
        "\n",
        "# Create output directories if not exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(cropped_image_path, exist_ok=True)\n",
        "os.makedirs(cropped_image_val_path, exist_ok=True)\n",
        "os.makedirs(annotated_output_path, exist_ok=True)\n",
        "output_directories_name = [\"output_path\", \"cropped_image_path\", \"cropped_image_val_path\", \"annotated_output_path\"]\n",
        "output_directories = [output_path, cropped_image_path, cropped_image_val_path, annotated_output_path]\n",
        "print(\"\\nCreating output directories...\")\n",
        "for dir_name, dir_path in zip(output_directories_name, output_directories):\n",
        "    print(f\"‚úÖ {dir_name} created: {dir_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Paths setup\")"
      ],
      "metadata": {
        "id": "Pcs4MoAKOOoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse CVAT XML"
      ],
      "metadata": {
        "id": "8kIzCevTQpdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cvat_annotations(annotations_xml_path):\n",
        "    tree = ET.parse(annotations_xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = {}\n",
        "\n",
        "    for image in tqdm(root.findall('image')):\n",
        "        image_name = image.attrib['name']\n",
        "        boxes = []\n",
        "\n",
        "        for box in image.findall('box'):\n",
        "            label = box.attrib.get('label')\n",
        "            if label != 'carplate':\n",
        "                continue\n",
        "\n",
        "            xtl = float(box.attrib['xtl'])\n",
        "            ytl = float(box.attrib['ytl'])\n",
        "            xbr = float(box.attrib['xbr'])\n",
        "            ybr = float(box.attrib['ybr'])\n",
        "\n",
        "            plate_number = None\n",
        "            for attr in box.findall('attribute'):\n",
        "                if attr.attrib.get('name') == 'plate_number':\n",
        "                    plate_number = attr.text.strip() if attr.text else None\n",
        "                    break\n",
        "\n",
        "            boxes.append({\n",
        "                'bbox': (xtl, ytl, xbr, ybr),\n",
        "                'plate_number': plate_number\n",
        "            })\n",
        "        annotations[image_name] = boxes\n",
        "    return annotations\n",
        "    print(\"‚úÖ CVAT annotations for training parsed\")\n",
        "\n",
        "annotations = parse_cvat_annotations(training_annotations_xml_path)"
      ],
      "metadata": {
        "id": "vLt5K2BYusY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cvat_annotations(annotations_xml_path):\n",
        "    tree = ET.parse(annotations_xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = {}\n",
        "\n",
        "    for image in tqdm(root.findall('image')):\n",
        "        image_name = image.attrib['name']\n",
        "        boxes = []\n",
        "\n",
        "        for box in image.findall('box'):\n",
        "            label = box.attrib.get('label')\n",
        "            if label != 'carplate':\n",
        "                continue\n",
        "\n",
        "            xtl = float(box.attrib['xtl'])\n",
        "            ytl = float(box.attrib['ytl'])\n",
        "            xbr = float(box.attrib['xbr'])\n",
        "            ybr = float(box.attrib['ybr'])\n",
        "\n",
        "            plate_number = None\n",
        "            for attr in box.findall('attribute'):\n",
        "                if attr.attrib.get('name') == 'plate_number':\n",
        "                    plate_number = attr.text.strip() if attr.text else None\n",
        "                    break\n",
        "\n",
        "            boxes.append({\n",
        "                'bbox': (xtl, ytl, xbr, ybr),\n",
        "                'plate_number': plate_number\n",
        "            })\n",
        "        annotations[image_name] = boxes\n",
        "    return annotations\n",
        "    print(\"‚úÖ CVAT annotations for validation parsed\")\n",
        "\n",
        "annotations = parse_cvat_annotations(validation_annotations_xml_path)"
      ],
      "metadata": {
        "id": "EHVm7igHP0g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Cropped Images, Save All Labels and Bounding Boxes as CSV"
      ],
      "metadata": {
        "id": "pcWdulV5vUpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(cropped_image_path) or not os.path.exists(training_labels_csv_path):\n",
        "    print(\"Processing images and creating labels CSV...\")\n",
        "    data = []\n",
        "\n",
        "    for image_name, boxes in tqdm(annotations.items()):\n",
        "        image_path = os.path.join(training_image_path, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"‚ùå Image {image_path} not found, skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error opening image {image_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        for i, box_info in enumerate(boxes):\n",
        "            xtl, ytl, xbr, ybr = box_info['bbox']\n",
        "            plate_number = box_info['plate_number']\n",
        "\n",
        "            if not plate_number:\n",
        "                continue\n",
        "\n",
        "            # Crop the car plate\n",
        "            cropped = img.crop((xtl, ytl, xbr, ybr))\n",
        "\n",
        "            # Preserve original file extension\n",
        "            original_ext = os.path.splitext(image_name)[1]\n",
        "            cropped_filename = f\"{os.path.splitext(image_name)[0]}_{i}{original_ext}\"\n",
        "            cropped_path = os.path.join(cropped_image_path, cropped_filename)\n",
        "            cropped.save(cropped_path)\n",
        "\n",
        "            # Append to dataset\n",
        "            data.append({\n",
        "                'image': cropped_filename,\n",
        "                'xtl': xtl,\n",
        "                'ytl': ytl,\n",
        "                'xbr': xbr,\n",
        "                'ybr': ybr,\n",
        "                'plate_number': plate_number\n",
        "            })\n",
        "\n",
        "    print(f\"\\n‚úÖ Cropped {len(data)} images and saved to {cropped_image_path}\")\n",
        "\n",
        "    # Save CSV file\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(training_labels_csv_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Saved labels and bounding boxes to {training_labels_csv_path}\")\n",
        "else:\n",
        "    print(\"‚úÖ Training labels CSV and cropped training images already exist.\")"
      ],
      "metadata": {
        "id": "HVgzwDjdQ5uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(cropped_image_val_path) or not os.path.exists(validation_labels_csv_path):\n",
        "    print(\"Processing images and creating labels CSV...\")\n",
        "    data = []\n",
        "\n",
        "    for image_name, boxes in tqdm(annotations.items()):\n",
        "        image_path = os.path.join(validation_image_path, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"‚ùå Image {image_path} not found, skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error opening image {image_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        for i, box_info in enumerate(boxes):\n",
        "            xtl, ytl, xbr, ybr = box_info['bbox']\n",
        "            plate_number = box_info['plate_number']\n",
        "\n",
        "            if not plate_number:\n",
        "                continue\n",
        "\n",
        "            # Crop the car plate\n",
        "            cropped = img.crop((xtl, ytl, xbr, ybr))\n",
        "\n",
        "            # Preserve original file extension\n",
        "            original_ext = os.path.splitext(image_name)[1]\n",
        "            cropped_filename = f\"{os.path.splitext(image_name)[0]}_{i}{original_ext}\"\n",
        "            cropped_path = os.path.join(cropped_image_val_path, cropped_filename)\n",
        "            cropped.save(cropped_path)\n",
        "\n",
        "            # Append to dataset\n",
        "            data.append({\n",
        "                'image': cropped_filename,\n",
        "                'xtl': xtl,\n",
        "                'ytl': ytl,\n",
        "                'xbr': xbr,\n",
        "                'ybr': ybr,\n",
        "                'plate_number': plate_number\n",
        "            })\n",
        "\n",
        "    print(f\"\\n‚úÖ Cropped {len(data)} images and saved to {cropped_image_val_path}\")\n",
        "\n",
        "    # Save CSV file\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(validation_labels_csv_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Saved labels and bounding boxes to {validation_labels_csv_path}\")\n",
        "else:\n",
        "    print(\"‚úÖ Validation labels CSV and cropped validation images already exist.\")"
      ],
      "metadata": {
        "id": "vRPv5uHzN-B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-parameters"
      ],
      "metadata": {
        "id": "WNV1qUZu9dXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 50\n",
        "patience_epochs = 10\n",
        "epochs_no_improve = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "print(\"\\n‚úÖ Hyper-parameters set\")"
      ],
      "metadata": {
        "id": "8_hwIBBtesg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Charset"
      ],
      "metadata": {
        "id": "D9dt_IXJ-Dnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\" # Removed space, handle spaces in pre/post processing if needed\n",
        "char_to_idx = {char: idx + 1 for idx, char in enumerate(ALPHABET)}  # CTC: 0 is blank\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "num_classes = len(ALPHABET) + 1  # +1 for CTC blank\n",
        "\n",
        "def encode_label(text):\n",
        "    text = str(text).upper().replace(\" \", \"\") # Normalize before encoding\n",
        "    return [char_to_idx[c] for c in text if c in char_to_idx]\n",
        "\n",
        "print(\"‚úÖ Charset defined\")"
      ],
      "metadata": {
        "id": "q7kQ42GQesg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Malaysia Car License Plate Dataset"
      ],
      "metadata": {
        "id": "FtOfgmXj-UHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNNCarPlateDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_folder, transform=None):\n",
        "        self.labels_df = pd.read_csv(csv_path)\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform if transform else transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labels_df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_folder, row['image'])\n",
        "        label_text = str(row['plate_number'])\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('L')  # grayscale\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Encode label\n",
        "        label_seq = torch.tensor(encode_label(label_text), dtype=torch.long)\n",
        "\n",
        "        return image, label_seq, label_text\n",
        "\n",
        "print(\"‚úÖ Dataset defined\")"
      ],
      "metadata": {
        "id": "dDJgG_e-esg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRNN Model Architecture"
      ],
      "metadata": {
        "id": "RrfW1TwQ_kcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self, img_height, num_classes):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1),  # input_channels, output_channels, kernel_size\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # height becomes 32/2 = 16\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # height becomes 16/2 = 8\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,1), (2,1)), # height becomes 8/2 = 4\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            # Adjust the last pooling layer to reduce height from 4 to 1\n",
        "            nn.MaxPool2d((4,1), (4,1)) # kernel_size=(height, width), stride=(height, width)\n",
        "        )\n",
        "        # The input size to the RNN needs to be recalculated based on the new CNN output dimensions.\n",
        "        # If input is (B, 1, 32, 128), after CNN with new pooling:\n",
        "        # Output channels = 512\n",
        "        # Width calculation: 128 -> 64 -> 32 -> 32 -> 32 (pooling only affects width by /2 twice)\n",
        "        # Height calculation: 32 -> 16 -> 8 -> 4 -> 1 (pooling affects height by /2 four times)\n",
        "        # Final feature map size: (B, 512, 1, 32)\n",
        "        # Input to RNN: (Batch, Seq_len, Features) = (B, Width, Channels * Height) = (B, 32, 512 * 1) = (B, 32, 512)\n",
        "        # The RNN expects 512 features per step.\n",
        "        self.rnn = nn.LSTM(512, 256, bidirectional=True, num_layers=2, batch_first=True)\n",
        "        # The FC layer input size should be 2 * hidden_size (due to bidirectional)\n",
        "        self.fc = nn.Linear(2 * 256, num_classes) # 2*256 because of bidirectional=True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x) # x shape: (batch, channels, height, width) e.g. B, 512, 1, 32 for h=32, w=128 input\n",
        "        b, c, h, w = x.size()\n",
        "        # For CRNN, LSTM expects (batch, seq_len, features)\n",
        "        # Here, width is sequence length. Features are channels * height\n",
        "        # The assertion should now pass if the input height is 32\n",
        "        assert h == 1, f\"Feature map height expected to be 1, but got {h}\"\n",
        "        x = x.squeeze(2) # Remove height dim: (b, c, w)\n",
        "        x = x.permute(0, 2, 1) # (b, w, c) which is (batch, seq_len, num_features) for LSTM. c is now 512\n",
        "        x, _ = self.rnn(x) # x shape (B, W, 2*hidden_size) -> (B, 32, 512)\n",
        "        x = self.fc(x) # (B, W, num_classes) -> (B, 32, num_classes)\n",
        "        x = x.permute(1, 0, 2)  # (w, b, num_classes) for CTC Loss: (T, N, C) -> (32, B, num_classes)\n",
        "        return x\n",
        "\n",
        "print(\"‚úÖ CRNN model defined\")"
      ],
      "metadata": {
        "id": "3jeUXuX0esg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders, Train and Testing Sets"
      ],
      "metadata": {
        "id": "6_mV_Rb8B_05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CRNN Training Transform (can add more augmentation here if retraining)\n",
        "crnn_train_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),\n",
        "    # Add more augmentations here for CRNN training if desired:\n",
        "    # transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),\n",
        "    # Apply geometric transformations\n",
        "    transforms.RandomAffine(degrees=(-5, 5),      # Rotate by -5 to +5 degrees\n",
        "                            translate=(0.05, 0.05), # Translate by up to 5% of width/height\n",
        "                            scale=(0.9, 1.1),     # Scale by 90% to 110%\n",
        "                            shear=(-5, 5)),       # Shear by -5 to +5 degrees\n",
        "    # transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    # Apply color/pixel-level transformations (your images are grayscale, so brightness/contrast are most relevant)\n",
        "    transforms.ColorJitter(brightness=(0.7, 1.3), # Randomly change brightness (e.g., 70% to 130% of original)\n",
        "                           contrast=(0.7, 1.3)),   # Randomly change contrast\n",
        "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    # Optional: Gaussian Blur (can sometimes make characters harder to read, use with caution)\n",
        "    # If you use it, start with small values:\n",
        "    # transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# CRNN Inference Transform (simpler, no augmentation)\n",
        "crnn_inference_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = CRNNCarPlateDataset(training_labels_csv_path, cropped_image_path, transform=crnn_train_transform)\n",
        "# Training dataset split (5353 cropped training images): 80% for training, 20% for validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
        "\n",
        "print(f\"Train set size: {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "print(\"\\n‚úÖ Dataloaders, Training and Validation Sets defined\")"
      ],
      "metadata": {
        "id": "dPqDcZf2esg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "NSnCRQHhDDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_output(outputs_from_model):\n",
        "    \"\"\"Greedy decoding for CRNN output during training/validation.\"\"\"\n",
        "    probs = torch.softmax(outputs_from_model, 2)\n",
        "    argmax_probs, argmax_indices = probs.max(2)\n",
        "    argmax_indices = argmax_indices.permute(1,0) # (B, T)\n",
        "    pred_texts = []\n",
        "    for i in range(argmax_indices.size(0)):\n",
        "        path = argmax_indices[i]\n",
        "        text = ''\n",
        "        last_idx = 0\n",
        "        for char_idx_tensor in path:\n",
        "            char_idx = char_idx_tensor.item()\n",
        "            if char_idx != 0 and char_idx != last_idx: # Not blank and not repeated\n",
        "                if char_idx in idx_to_char:\n",
        "                    text += idx_to_char[char_idx]\n",
        "            last_idx = char_idx\n",
        "        pred_texts.append(text)\n",
        "    return pred_texts\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device=device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for batch in tqdm(loader, desc=\"Training Epoch\", leave=True):\n",
        "        imgs, labels, _ = zip(*batch)\n",
        "        imgs = torch.stack(imgs).to(device)\n",
        "        labels_concat = torch.cat(labels).to(device)\n",
        "\n",
        "        outputs = model(imgs) # (T, B, C)\n",
        "        output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
        "        target_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs.log_softmax(2), labels_concat, output_lengths, target_lengths) # log_softmax for CTCLoss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def validate_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0; total_correct_seq = 0; total_seq = 0; total_correct_chars = 0; total_chars = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Validating Epoch\", leave=True):\n",
        "            imgs, labels, label_texts = zip(*batch)\n",
        "            imgs = torch.stack(imgs).to(device)\n",
        "            labels_concat = torch.cat(labels).to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
        "            target_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long).to(device)\n",
        "\n",
        "            loss = criterion(outputs.log_softmax(2), labels_concat, output_lengths, target_lengths)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = decode_output(outputs) # Use original decode_output here\n",
        "            for pred, true_text in zip(preds, label_texts):\n",
        "                true_text_norm = true_text.upper().replace(\" \",\"\")\n",
        "                if pred.strip() == true_text_norm.strip(): total_correct_seq +=1\n",
        "                total_seq +=1\n",
        "                correct_chars_count = sum(p == t for p, t in zip(pred, true_text_norm))\n",
        "                total_correct_chars += correct_chars_count\n",
        "                total_chars += len(true_text_norm)\n",
        "    seq_acc = (total_correct_seq / total_seq) if total_seq > 0 else 0\n",
        "    char_acc = (total_correct_chars / total_chars) if total_chars > 0 else 0\n",
        "    return running_loss / len(loader), seq_acc, char_acc\n",
        "\n",
        "print(\"‚úÖ Training Model defined\")"
      ],
      "metadata": {
        "id": "vM95EqTnesg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Model, Loss, Optimizer"
      ],
      "metadata": {
        "id": "TK81BGSYEX_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CRNN(img_height=32, num_classes=num_classes).to(device)\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "print(\"‚úÖ Model, Loss, Optimizer initialized\")"
      ],
      "metadata": {
        "id": "uvxmaIaH0KTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_seq_accuracies = []\n",
        "val_char_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_seq_acc, val_char_acc = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_seq_accuracies.append(val_seq_acc)\n",
        "    val_char_accuracies.append(val_char_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Seq Acc: {val_seq_acc*100:.2f}% | Val Char Acc: {val_char_acc*100:.2f}%\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        epochs_no_improve = 0\n",
        "        print(f\"‚úÖ Saved Best Model at epoch {epoch}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience_epochs:\n",
        "        print(f\"Validation loss did not improve for {patience_epochs} epochs. Stopping early\")\n",
        "        break\n",
        "\n",
        "print(\"\\nüéâ CRNN training complete!\")"
      ],
      "metadata": {
        "id": "KFHyBsgPesg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Loss Curves and Accuracy Curves"
      ],
      "metadata": {
        "id": "NB9RoW4jE-lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CTC Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(output_path, 'LossCurves.png'))\n",
        "plt.show()\n",
        "print(\"\\n‚úÖ Loss curves plotted and saved\")"
      ],
      "metadata": {
        "id": "a441yCvrEVdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(val_seq_accuracies, label=\"Val Seq Accuracy\")\n",
        "plt.plot(val_char_accuracies, label=\"Val Char Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy (Sequence & Character)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_path, 'AccuracyCurves.png'))\n",
        "plt.show()\n",
        "print(\"\\n‚úÖ Accuracy curves plotted and saved\")"
      ],
      "metadata": {
        "id": "XPgZRI3LcUQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on Test Set and Save Predictions to CSV"
      ],
      "metadata": {
        "id": "9DIYrKNhFpuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Best Model for Inference\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "KpeOfbVpWUYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_and_save_plate_predictions_with_individual_metrics( # Renamed for clarity\n",
        "    cropped_image_path,\n",
        "    csv_path,\n",
        "    output_csv_path,\n",
        "    model,\n",
        "    transform,\n",
        "    device,\n",
        "    decode_output_fn\n",
        "):\n",
        "    \"\"\"\n",
        "    Displays EVERY image with its true vs predicted label and individual outcome (match, edit distance).\n",
        "    Saves all prediction results to CSV, and calculates overall CRNN inference performance metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def predict_plate(image_path):\n",
        "        image = Image.open(image_path).convert('L')\n",
        "        img = transform(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(img)\n",
        "            decoded_results = decode_output_fn(outputs)\n",
        "            if isinstance(decoded_results, tuple): # Handle (texts, confs)\n",
        "                texts = decoded_results[0]\n",
        "            else: # Handle just texts\n",
        "                texts = decoded_results\n",
        "        return texts[0] if texts else \"\"\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    predictions_data = []\n",
        "\n",
        "    # --- Overall Metrics Initialization ---\n",
        "    total_plates_processed = 0\n",
        "    overall_correct_exact_matches = 0\n",
        "    overall_total_edit_distance = 0\n",
        "    overall_total_true_char_length = 0\n",
        "    # --- End Overall Metrics Initialization ---\n",
        "\n",
        "    print(f\"Processing and displaying all {len(df)} images with individual metrics...\\n\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating & Displaying All Images\"):\n",
        "        image_name = row['image']\n",
        "        true_label_raw = str(row['plate_number'])\n",
        "        image_path = os.path.join(cropped_image_path, image_name)\n",
        "\n",
        "        predicted_label_raw = \"\"\n",
        "        is_exact_match_current_image = False\n",
        "        edit_distance_current_image = -1 # Default if not applicable\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"‚ö†Ô∏è Image {image_path} not found for {image_name}, skipping display and metrics for this entry.\")\n",
        "            predictions_data.append({\n",
        "                'image': image_name,\n",
        "                'true_label': true_label_raw,\n",
        "                'predicted_label': \"IMAGE_NOT_FOUND\",\n",
        "                'is_exact_match': False,\n",
        "                'edit_distance': -1\n",
        "            })\n",
        "            # Display a placeholder or skip if you prefer\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.text(0.5, 0.5, f\"Image Not Found:\\n{image_name}\", ha='center', va='center', fontsize=12, color='red')\n",
        "            plt.title(f\"File: {image_name}\", fontsize=10)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            continue\n",
        "\n",
        "        predicted_label_raw = predict_plate(image_path)\n",
        "        total_plates_processed += 1\n",
        "\n",
        "        true_label_norm = true_label_raw.upper().replace(\" \", \"\")\n",
        "        predicted_label_norm = predicted_label_raw.upper().replace(\" \", \"\")\n",
        "\n",
        "        # Individual image metrics\n",
        "        if true_label_norm == predicted_label_norm:\n",
        "            is_exact_match_current_image = True\n",
        "            overall_correct_exact_matches += 1\n",
        "\n",
        "        if true_label_norm: # Calculate edit distance if there's a ground truth\n",
        "            edit_distance_current_image = levenshtein_distance(predicted_label_norm, true_label_norm)\n",
        "            overall_total_edit_distance += edit_distance_current_image\n",
        "            overall_total_true_char_length += len(true_label_norm)\n",
        "        else: # No ground truth to compare against for edit distance\n",
        "            edit_distance_current_image = 0 # Or some indicator that it's not applicable\n",
        "\n",
        "        # --- Display Image with Individual Metric Info ---\n",
        "        try:\n",
        "            image_pil = Image.open(image_path)\n",
        "            fig, ax = plt.subplots(figsize=(6, 4))\n",
        "            ax.imshow(image_pil, cmap='gray')\n",
        "\n",
        "            title_color = 'green' if is_exact_match_current_image else 'red'\n",
        "            title_text = f\"File: {image_name}\\nTrue: {true_label_raw} | Predicted: {predicted_label_raw}\"\n",
        "            ax.set_title(title_text, fontsize=12, color=title_color, pad=20)\n",
        "\n",
        "            # Add text below the image for individual metrics\n",
        "            info_text = f\"Exact Match: {'YES' if is_exact_match_current_image else 'NO'}\\n\"\n",
        "            if true_label_norm: # Only show edit distance if comparable\n",
        "                info_text += f\"Edit Distance: {edit_distance_current_image}\"\n",
        "                if len(true_label_norm) > 0:\n",
        "                    char_err_rate_instance = (edit_distance_current_image / len(true_label_norm)) * 100\n",
        "                    info_text += f\" (Instance CER: {char_err_rate_instance:.2f}%)\"\n",
        "            else:\n",
        "                info_text += \"Edit Distance: N/A (No ground truth for comparison)\"\n",
        "\n",
        "            # Position text - you might need to adjust coordinates based on your image sizes\n",
        "            # This places it relative to the axes, below the image\n",
        "            plt.text(0.5, -0.15, info_text, ha='center', va='top', transform=ax.transAxes,\n",
        "                     fontsize=10, bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "            ax.axis('off')\n",
        "            plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust layout to make space for title and text\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying image {image_name}: {e}\")\n",
        "        # --- End Display Image ---\n",
        "\n",
        "        predictions_data.append({\n",
        "            'image': image_name,\n",
        "            'true_label': true_label_raw,\n",
        "            'predicted_label': predicted_label_raw,\n",
        "            'is_exact_match': is_exact_match_current_image,\n",
        "            'edit_distance': edit_distance_current_image,\n",
        "            'true_label_normalized': true_label_norm,\n",
        "            'predicted_label_normalized': predicted_label_norm\n",
        "        })\n",
        "\n",
        "    # Save all predictions to CSV\n",
        "    pred_df = pd.DataFrame(predictions_data)\n",
        "    pred_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"\\n‚úÖ Saved all predictions to {output_csv_path}\")\n",
        "\n",
        "    # --- Calculate and Print Final Overall Metrics ---\n",
        "    overall_exact_match_accuracy = 0\n",
        "    overall_cer = float('nan')\n",
        "    overall_normalized_char_accuracy = float('nan')\n",
        "\n",
        "    print(f\"\\n-------------- Overall CRNN Inference Performance Metrics --------------\")\n",
        "    if total_plates_processed > 0:\n",
        "        overall_exact_match_accuracy = (overall_correct_exact_matches / total_plates_processed) * 100\n",
        "        print(f\"Total Plates Processed & Displayed: {total_plates_processed}\")\n",
        "        print(f\"Overall Exact Match Accuracy (Sequence Accuracy): {overall_exact_match_accuracy:.2f}% ({overall_correct_exact_matches}/{total_plates_processed})\")\n",
        "\n",
        "        if overall_total_true_char_length > 0:\n",
        "            overall_cer = (overall_total_edit_distance / overall_total_true_char_length) * 100\n",
        "            overall_normalized_char_accuracy = (1 - (overall_total_edit_distance / overall_total_true_char_length)) * 100\n",
        "            print(f\"Overall Average Character Error Rate (CER): {overall_cer:.2f}%\")\n",
        "            print(f\"Overall Normalized Character Accuracy: {overall_normalized_char_accuracy:.2f}%\")\n",
        "        else:\n",
        "            print(\"Overall Character Error Rate (CER) and Normalized Character Accuracy could not be calculated (no true characters found or processed).\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå No plates were processed to calculate overall metrics.\")\n",
        "    print(f\"-----------------------------------------------------------------------\")\n",
        "\n",
        "    return {\n",
        "        \"total_plates_processed\": total_plates_processed,\n",
        "        \"overall_exact_match_accuracy\": overall_exact_match_accuracy,\n",
        "        \"overall_cer\": overall_cer,\n",
        "        \"overall_normalized_char_accuracy\": overall_normalized_char_accuracy\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ show_and_save_plate_predictions_with_individual_metrics defined\")"
      ],
      "metadata": {
        "id": "P9ra5QyeFxhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting CRNN inference evaluation with individual display and metrics...\")\n",
        "# Note: This will display ALL images from your training_labels_csv_path\n",
        "inference_metrics_individual_display = show_and_save_plate_predictions_with_individual_metrics(\n",
        "    cropped_image_path=cropped_image_val_path,\n",
        "    csv_path=validation_labels_csv_path,\n",
        "    output_csv_path=predictions_csv_path, # This will overwrite the previous one\n",
        "    model=model,\n",
        "    transform=crnn_inference_transform,\n",
        "    device=device,\n",
        "    decode_output_fn=decode_output # Or decode_output_with_confidence\n",
        ")\n",
        "\n",
        "print(\"\\nReturned Overall Metrics Dictionary:\")\n",
        "print(inference_metrics_individual_display)\n",
        "print(\"\\n‚úÖ CRNN inference evaluation with individual display and metrics completed\")"
      ],
      "metadata": {
        "id": "OJMsqNa4f2QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection and CRNN Inference"
      ],
      "metadata": {
        "id": "5ASTkfF0D_Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_plausible_malaysian_plate(text, min_len=3, max_len=10):\n",
        "    \"\"\"Basic filter for plausible Malaysian license plate text.\"\"\"\n",
        "    if not text or not isinstance(text, str) or not (min_len <= len(text) <= max_len):\n",
        "        return False\n",
        "    # Normalize: uppercase, remove ALL spaces for pattern matching and consistency\n",
        "    text_norm = text.upper().replace(\" \", \"\")\n",
        "    if not text_norm: return False # Empty after removing spaces\n",
        "\n",
        "    # Rule 1: Must contain at least one letter and one digit for most common plates\n",
        "    has_letter = any(c.isalpha() for c in text_norm)\n",
        "    has_digit = any(c.isdigit() for c in text_norm)\n",
        "\n",
        "    if not (has_letter and has_digit):\n",
        "        # Allow purely alphabetical special plates if they are reasonably long\n",
        "        # (e.g., \"PUTRAJAYA\", \"SUKOM\")\n",
        "        if text_norm.isalpha() and len(text_norm) >= 5:\n",
        "            # Could check against a known list of special plates here\n",
        "            pass # Allow\n",
        "        else:\n",
        "            return False # Likely not a standard plate\n",
        "\n",
        "    # Rule 2: Reject if too many consecutive identical characters (often CRNN misreads like \"AAAAA\" or \"11111\")\n",
        "    if re.search(r'(.)\\1{3,}', text_norm):  # 4 or more consecutive same characters\n",
        "        return False\n",
        "\n",
        "    # Rule 3: All characters must be in our defined ALPHABET (after normalization)\n",
        "    for char_p in text_norm:\n",
        "        if char_p not in ALPHABET: # ALPHABET here does not have space\n",
        "            return False\n",
        "\n",
        "    # Rule 4: Check typical Malaysian plate patterns (simplified)\n",
        "    # Common: LLLDDDD, LL DDDD, L DDDD, LLL DDD, etc. (L=Letter, D=Digit)\n",
        "    # Vowels 'I' and 'O' are often confused with '1' and '0'. CRNN might output them.\n",
        "    # This rule helps catch some structural anomalies but is not exhaustive.\n",
        "    # Pattern: 1-3 leading letters, then 1-4 digits, optionally followed by 1-2 trailing letters.\n",
        "    m = re.match(r'^([A-Z]{1,3})(\\d{1,4})([A-Z]{0,2})$', text_norm)\n",
        "    # Special prefixes like S (Sabah), Q (Sarawak), J (Johor) etc. are covered by [A-Z]\n",
        "    # This regex is quite strict for standard plates.\n",
        "    # Example special plates: G1M, IM4U, PATRIOT, NBOS, etc.\n",
        "    # Example taxi: HWA1234\n",
        "\n",
        "    # Looser check: starts with letter, ends with digit or letter.\n",
        "    if not text_norm[0].isalpha():\n",
        "        return False # Most plates start with a letter\n",
        "\n",
        "    if not (text_norm[-1].isdigit() or text_norm[-1].isalpha()): # Should end in digit or letter\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "print(\"‚úÖ Plausible Malaysian plate filter function defined.\")"
      ],
      "metadata": {
        "id": "ngcpwYmHyEhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_output_with_confidence(outputs_from_model, char_map=idx_to_char):\n",
        "    \"\"\"\n",
        "    Greedy decoding with a simple confidence score from CRNN output.\n",
        "    Confidence is the mean probability of the selected (non-blank, non-repeated) characters.\n",
        "    \"\"\"\n",
        "    # outputs_from_model shape: (Sequence_Length, Batch_Size, Num_Classes)\n",
        "    # Ensure it's on CPU for numpy operations if needed, and detach from graph\n",
        "    outputs_from_model = outputs_from_model.cpu().detach()\n",
        "\n",
        "    log_probs = torch.nn.functional.log_softmax(outputs_from_model, dim=2)\n",
        "    probs = torch.exp(log_probs) # Probabilities (T, B, C)\n",
        "\n",
        "    best_path_probs_per_step, best_path_indices = probs.max(2) # (T, B)\n",
        "\n",
        "    # Permute to (B, T) for easier iteration over batch\n",
        "    best_path_indices = best_path_indices.permute(1, 0)\n",
        "    best_path_probs_per_step = best_path_probs_per_step.permute(1, 0)\n",
        "\n",
        "    pred_texts = []\n",
        "    pred_confidences = []\n",
        "\n",
        "    for i in range(best_path_indices.size(0)): # Iterate over batch\n",
        "        batch_item_path_indices = best_path_indices[i]\n",
        "        batch_item_step_probs = best_path_probs_per_step[i]\n",
        "\n",
        "        decoded_text = ''\n",
        "        char_probabilities_for_text = []\n",
        "        last_char_idx = 0\n",
        "\n",
        "        for j, current_char_idx_tensor in enumerate(batch_item_path_indices):\n",
        "            current_char_idx = current_char_idx_tensor.item()\n",
        "\n",
        "            if current_char_idx != 0 and current_char_idx != last_char_idx: # Not blank and not repeated\n",
        "                if current_char_idx in char_map:\n",
        "                    decoded_text += char_map[current_char_idx]\n",
        "                    char_probabilities_for_text.append(batch_item_step_probs[j].item())\n",
        "            last_char_idx = current_char_idx\n",
        "\n",
        "        pred_texts.append(decoded_text)\n",
        "        if char_probabilities_for_text:\n",
        "            # Geometric mean is better for product of probabilities, but arithmetic mean is simpler here.\n",
        "            # Or, sum of log-probabilities. For now, arithmetic mean of probabilities.\n",
        "            pred_confidences.append(np.mean(char_probabilities_for_text))\n",
        "        else:\n",
        "            pred_confidences.append(0.0) # No valid characters decoded\n",
        "\n",
        "    return pred_texts, pred_confidences\n",
        "\n",
        "print(\"‚úÖ decode_output_with_confidence defined\")"
      ],
      "metadata": {
        "id": "k0-dM8ekyO-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(boxA, boxB):\n",
        "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # Compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "    # Compute the area of both the prediction and ground-truth rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "\n",
        "    # Compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the intersection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    return iou\n",
        "\n",
        "print(\"‚úÖ IoU calculation function defined\")"
      ],
      "metadata": {
        "id": "kSAjf6B2nRi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_od_model(model_path, device):\n",
        "    \"\"\"Loads the YOLO object detection model.\"\"\"\n",
        "    try:\n",
        "        model = YOLO(model_path)\n",
        "        model.to(device)\n",
        "        print(f\"‚úÖ Object detection model loaded successfully from {model_path} on {device}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading object detection model: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_tf_saved_model(model_dir_path):\n",
        "    \"\"\"Loads a TensorFlow SavedModel for inference.\"\"\"\n",
        "    try:\n",
        "        model = tf.saved_model.load(model_dir_path)\n",
        "        # Often, the actual inference function is a concrete function.\n",
        "        # You might need to explore `model.signatures` to find the correct one.\n",
        "        # Common signature is 'serving_default'.\n",
        "        infer = model.signatures[\"serving_default\"]\n",
        "        print(f\"‚úÖ TensorFlow SavedModel loaded successfully from {model_dir_path} on {device}\")\n",
        "        return infer # Return the inference function\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading TensorFlow SavedModel: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_crnn_for_inference(crnn_model_path, img_h, num_cls, device_to_use):\n",
        "    try:\n",
        "        model_crnn_inf = CRNN(img_height=img_h, num_classes=num_cls)\n",
        "        if not os.path.exists(crnn_model_path):\n",
        "            print(f\"‚ùå CRNN model file not found at {crnn_model_path}. Cannot load for inference.\")\n",
        "            return None\n",
        "        model_crnn_inf.load_state_dict(torch.load(crnn_model_path, map_location=device_to_use))\n",
        "        model_crnn_inf.to(device_to_use)\n",
        "        model_crnn_inf.eval()\n",
        "        print(f\"‚úÖ CRNN model loaded successfully from {crnn_model_path} for inference on {device_to_use}\")\n",
        "        return model_crnn_inf\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading CRNN model for inference: {e}\")\n",
        "        return None\n",
        "\n",
        "def predict_plate_with_crnn_and_conf(cropped_pil_image, model_crnn_inf, transform_crnn, device_to_use):\n",
        "    \"\"\" Recognizes characters and returns text with confidence. \"\"\"\n",
        "    if cropped_pil_image.mode != 'L':\n",
        "        image_l = cropped_pil_image.convert('L')\n",
        "    else:\n",
        "        image_l = cropped_pil_image\n",
        "    if image_l.width == 0 or image_l.height == 0: # Handle empty crops\n",
        "        return \"N/A\", 0.0\n",
        "\n",
        "    image_tensor = transform_crnn(image_l).unsqueeze(0).to(device_to_use)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_crnn_inf(image_tensor) # (T, B, C), B=1\n",
        "        pred_texts_list, pred_confidences_list = decode_output_with_confidence(outputs) # Use new decoder\n",
        "\n",
        "    text_result = pred_texts_list[0] if pred_texts_list else \"N/A\"\n",
        "    conf_result = pred_confidences_list[0] if pred_confidences_list else 0.0\n",
        "\n",
        "    # Normalize text output (remove spaces, uppercase) for consistency\n",
        "    text_result = text_result.upper().replace(\" \", \"\")\n",
        "\n",
        "    return text_result, conf_result\n",
        "\n",
        "print(\"‚úÖ Helper functions for OD and CRNN inference pipeline defined\")"
      ],
      "metadata": {
        "id": "rqp0B8NaDsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW/UPDATED FUNCTION: Run EfficientDet Inference and Parse Output\n",
        "def run_efficientdet_inference(tf_infer_function, pil_image,\n",
        "                               expected_input_size=(512, 512),\n",
        "                               score_threshold=0.40, # << INCREASED DEFAULT THRESHOLD\n",
        "                               license_plate_class_id=1, # VERIFY THIS\n",
        "                               min_plate_width=30,     # New: min expected width of a detected plate in pixels\n",
        "                               min_plate_height=10,    # New: min expected height\n",
        "                               max_aspect_ratio_dev=1.5):# New: How much aspect ratio can deviate (e.g., actual_ar / expected_ar)\n",
        "    \"\"\"\n",
        "    Runs inference using a loaded TensorFlow EfficientDet model and parses its output.\n",
        "    Returns a list of detection result structures compatible with the existing pipeline.\n",
        "    \"\"\"\n",
        "    original_img_width, original_img_height = pil_image.size\n",
        "\n",
        "    # Preprocessing: Resize and Pad to expected_input_size\n",
        "    img_resized_pil = pil_image.copy()\n",
        "    # Calculate aspect ratios\n",
        "    original_aspect = original_img_width / original_img_height\n",
        "    target_aspect = expected_input_size[0] / expected_input_size[1]\n",
        "\n",
        "    if original_aspect > target_aspect: # Original is wider than target aspect\n",
        "        new_width = expected_input_size[0]\n",
        "        new_height = int(new_width / original_aspect)\n",
        "    else: # Original is taller or same aspect\n",
        "        new_height = expected_input_size[1]\n",
        "        new_width = int(new_height * original_aspect)\n",
        "\n",
        "    img_resized_pil = img_resized_pil.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "    padded_img_pil = Image.new(\"RGB\", expected_input_size, (128, 128, 128)) # Pad with gray\n",
        "    paste_x = (expected_input_size[0] - img_resized_pil.width) // 2\n",
        "    paste_y = (expected_input_size[1] - img_resized_pil.height) // 2\n",
        "    padded_img_pil.paste(img_resized_pil, (paste_x, paste_y))\n",
        "\n",
        "    image_np_uint8 = np.array(padded_img_pil, dtype=np.uint8)\n",
        "    input_tensor = tf.convert_to_tensor(image_np_uint8)\n",
        "    input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "    detections = tf_infer_function(input_tensor=input_tensor) # TF Serving signature often uses named inputs\n",
        "\n",
        "    num_detections = int(detections['num_detections'][0])\n",
        "    det_boxes_normalized = detections['detection_boxes'][0,:num_detections].numpy()\n",
        "    det_scores = detections['detection_scores'][0,:num_detections].numpy()\n",
        "    det_classes = detections['detection_classes'][0,:num_detections].numpy().astype(np.int32)\n",
        "\n",
        "    output_boxes_for_yolo_structure = []\n",
        "    for i in range(num_detections):\n",
        "        score = det_scores[i]\n",
        "        class_id = det_classes[i]\n",
        "\n",
        "        if score >= score_threshold and class_id == license_plate_class_id:\n",
        "            ymin, xmin, ymax, xmax = det_boxes_normalized[i]\n",
        "\n",
        "            # Denormalize coordinates from padded 512x512 input\n",
        "            # to coordinates on the original image\n",
        "\n",
        "            # Coords relative to the 512x512 padded input:\n",
        "            pad_abs_xmin = xmin * expected_input_size[0]\n",
        "            pad_abs_ymin = ymin * expected_input_size[1]\n",
        "            pad_abs_xmax = xmax * expected_input_size[0]\n",
        "            pad_abs_ymax = ymax * expected_input_size[1]\n",
        "\n",
        "            # Subtract padding offset\n",
        "            unpadded_xmin = pad_abs_xmin - paste_x\n",
        "            unpadded_ymin = pad_abs_ymin - paste_y\n",
        "            unpadded_xmax = pad_abs_xmax - paste_x\n",
        "            unpadded_ymax = pad_abs_ymax - paste_y\n",
        "\n",
        "            # Scale back to original image dimensions\n",
        "            # (resized_pil dimensions are new_width, new_height)\n",
        "            original_xmin = int(unpadded_xmin * (original_img_width / new_width))\n",
        "            original_ymin = int(unpadded_ymin * (original_img_height / new_height))\n",
        "            original_xmax = int(unpadded_xmax * (original_img_width / new_width))\n",
        "            original_ymax = int(unpadded_ymax * (original_img_height / new_height))\n",
        "\n",
        "            # Clamp to original image boundaries\n",
        "            original_xmin = max(0, original_xmin)\n",
        "            original_ymin = max(0, original_ymin)\n",
        "            original_xmax = min(original_img_width -1 , original_xmax) # -1 to be safe for width/height indexing\n",
        "            original_ymax = min(original_img_height -1, original_ymax)\n",
        "\n",
        "\n",
        "            class FakeBox:\n",
        "                def __init__(self, xyxy_val, conf_val, cls_val):\n",
        "                    self.xyxy = [torch.tensor(xyxy_val, dtype=torch.float32)]\n",
        "                    self.conf = [torch.tensor(conf_val, dtype=torch.float32)]\n",
        "                    self.cls = [torch.tensor(cls_val, dtype=torch.float32)]\n",
        "\n",
        "            output_boxes_for_yolo_structure.append(\n",
        "                FakeBox([original_xmin, original_ymin, original_xmax, original_ymax], score, class_id)\n",
        "            )\n",
        "\n",
        "    class FakeResults:\n",
        "        def __init__(self, boxes_list):\n",
        "            self.boxes = boxes_list\n",
        "\n",
        "    return [FakeResults(output_boxes_for_yolo_structure)] if output_boxes_for_yolo_structure else [FakeResults([])]\n",
        "\n",
        "print(\"‚úÖ EfficientDet inference and parsing function defined\")"
      ],
      "metadata": {
        "id": "cRJj8NZcaMJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Processing"
      ],
      "metadata": {
        "id": "xn4pakjuENqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_image(image_path, od_model_inf_callable, current_od_model_type: str,\n",
        "                         crnn_model_inf, crnn_image_transform_inf, device_to_use, output_dir_img):\n",
        "    \"\"\"\n",
        "    Detects license plates in a single image, recognizes characters,\n",
        "    draws bounding boxes and labels, displays, and saves the annotated image.\n",
        "    Includes checks to keep text within image bounds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img_pil = Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading image {image_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Object Detection Call ---\n",
        "    results_od = None\n",
        "    if current_od_model_type.startswith(\"yolo\"):\n",
        "        results_od = od_model_inf_callable(img_pil, verbose=False, conf=0.25)\n",
        "    elif current_od_model_type == \"efficientdet\":\n",
        "        # For EfficientDet, the od_model_inf_callable IS the infer function.\n",
        "        # Preprocessing and postprocessing (including conf threshold) are in run_efficientdet_inference\n",
        "        results_od = run_efficientdet_inference(\n",
        "            tf_infer_function=od_model_inf_callable, # This is the loaded TF signature\n",
        "            pil_image=img_pil,\n",
        "            expected_input_size=(512, 512),      # As per your config\n",
        "            score_threshold=0.35,                # This is used INSIDE run_efficientdet_inference\n",
        "            license_plate_class_id=1             # VERIFY THIS\n",
        "        )\n",
        "    else:\n",
        "        print(f\"‚ùå Unknown OD model type for inference: {current_od_model_type}\")\n",
        "        return\n",
        "    # --- End Object Detection Call ---\n",
        "\n",
        "    annotated_img_pil = img_pil.copy() # Work on a copy for drawing\n",
        "    draw = ImageDraw.Draw(annotated_img_pil)\n",
        "    font_size = 28 # Increased font size\n",
        "    # Standard Colab/Linux font path for DejaVu Sans Bold\n",
        "    font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
        "    try:\n",
        "        font = ImageFont.truetype(font_path, font_size)\n",
        "    except IOError:\n",
        "        print(f\"‚ö†Ô∏è Font not found at {font_path}. Attempting to install fonts-dejavu...\")\n",
        "        # Commands to install fonts, ensure they run silently or handle output\n",
        "        os.system('apt-get update -qq > /dev/null')\n",
        "        os.system('apt-get install -qq -y fonts-dejavu > /dev/null')\n",
        "        print(\"‚úÖ DejaVu fonts installation attempt finished. Retrying font loading.\")\n",
        "        try:\n",
        "            font = ImageFont.truetype(font_path, font_size)\n",
        "            print(f\"‚úÖ Using installed font: {font_path}\")\n",
        "        except IOError:\n",
        "            print(\"‚ùå Failed to load DejaVuSans-Bold.ttf even after installation. Using default font.\")\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "\n",
        "    num_detections_drawn = 0\n",
        "    # The structure of results_od[0].boxes should now be consistent (list of FakeBox or YOLO's Box objects)\n",
        "    if results_od and results_od[0] and hasattr(results_od[0], 'boxes') and results_od[0].boxes:\n",
        "        for i, box_obj in enumerate(results_od[0].boxes): # box_obj is either yolo.Box or our FakeBox\n",
        "            # Access attributes directly or via index if they are lists of tensors\n",
        "            xyxy_tensor = box_obj.xyxy[0] # This is a tensor [xmin, ymin, xmax, ymax]\n",
        "            od_conf_tensor = box_obj.conf[0] # This is a tensor [confidence]\n",
        "            # cls_tensor = box_obj.cls[0] # If needed\n",
        "\n",
        "            xyxy = xyxy_tensor.cpu().numpy().astype(int)\n",
        "            od_conf = od_conf_tensor.cpu().numpy().item() # Get scalar value\n",
        "\n",
        "            xmin, ymin, xmax, ymax = xyxy\n",
        "            xmin = max(0, xmin); ymin = max(0, ymin)\n",
        "            xmax = min(img_pil.width, xmax); ymax = min(img_pil.height, ymax)\n",
        "\n",
        "            if xmax <= xmin or ymax <= ymin:\n",
        "                continue\n",
        "\n",
        "            cropped_plate_pil = img_pil.crop((xmin, ymin, xmax, ymax))\n",
        "\n",
        "            plate_text, plate_conf = \"N/A\", 0.0\n",
        "            if cropped_plate_pil.width > 5 and cropped_plate_pil.height > 5 : # Basic check for valid crop size\n",
        "                plate_text, plate_conf = predict_plate_with_crnn_and_conf(cropped_plate_pil, crnn_model_inf, crnn_image_transform_inf, device_to_use)\n",
        "\n",
        "            label = f\"{plate_text} (C:{plate_conf:.2f} | OD:{od_conf:.2f})\"\n",
        "            draw.rectangle([xmin, ymin, xmax, ymax], outline=\"green\", width=3) # Thicker box\n",
        "\n",
        "            # --- Text Placement and Boundary Check ---\n",
        "            text_x = xmin\n",
        "            text_y_above = ymin - font_size - 5 # Attempt to place above\n",
        "            text_y_below = ymax + 5 # Option to place below\n",
        "\n",
        "            # Calculate text bounding box assuming placement above\n",
        "            try:\n",
        "                text_bbox_above = draw.textbbox((text_x, text_y_above), label, font=font)\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ö†Ô∏è Warning: Could not calculate textbbox with font, using default. {e}\")\n",
        "                 # Fallback if textbbox calculation fails with the loaded font\n",
        "                 # This might not be perfectly accurate but prevents crashes\n",
        "                 default_font_size = 10 # Estimate default font size\n",
        "                 text_bbox_above = (text_x, text_y_above, text_x + len(label) * default_font_size * 0.6, text_y_above + default_font_size * 1.2)\n",
        "\n",
        "\n",
        "            # Check if placing above goes off the top edge\n",
        "            if text_bbox_above[1] < 0:\n",
        "                # Place below the box\n",
        "                text_y = text_y_below\n",
        "                try:\n",
        "                    text_bbox = draw.textbbox((text_x, text_y), label, font=font)\n",
        "                except Exception as e:\n",
        "                     print(f\"‚ö†Ô∏è Warning: Could not calculate textbbox with font, using default. {e}\")\n",
        "                     default_font_size = 10 # Estimate default font size\n",
        "                     text_bbox = (text_x, text_y, text_x + len(label) * default_font_size * 0.6, text_y + default_font_size * 1.2)\n",
        "            else:\n",
        "                # Place above the box\n",
        "                text_y = text_y_above\n",
        "                text_bbox = text_bbox_above\n",
        "\n",
        "            # Ensure text background stays within left/right image bounds\n",
        "            text_bbox_adjusted = list(text_bbox)\n",
        "            text_bbox_adjusted[0] = max(0, text_bbox_adjusted[0]) # Ensure left is not less than 0\n",
        "            text_bbox_adjusted[2] = min(img_pil.width, text_bbox_adjusted[2]) # Ensure right is not more than width\n",
        "\n",
        "            # Ensure text background stays within top/bottom image bounds\n",
        "            text_bbox_adjusted[1] = max(0, text_bbox_adjusted[1]) # Ensure top is not less than 0\n",
        "            text_bbox_adjusted[3] = min(img_pil.height, text_bbox_adjusted[3]) # Ensure bottom is not more than height\n",
        "\n",
        "            # Draw text background and text\n",
        "            draw.rectangle(text_bbox_adjusted, fill=\"green\")\n",
        "            draw.text((text_x, text_y), label, fill=\"black\", font=font)\n",
        "\n",
        "            print(f\"‚úÖ Plate {i+1}: Text='{plate_text}', CRNN_Conf={plate_conf:.2f}, OD_Conf={od_conf:.2f}, BBox={xyxy}\")\n",
        "            num_detections_drawn += 1\n",
        "\n",
        "    if num_detections_drawn == 0:\n",
        "        print(f\"‚ùå No license plates detected by OD in {os.path.basename(image_path)}.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Processed {num_detections_drawn} detections.\")\n",
        "\n",
        "    # Display in Colab (Matplotlib)\n",
        "    plt.figure(figsize=(20, 15)) # Larger figure for better visibility\n",
        "    plt.imshow(annotated_img_pil)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Annotated: {os.path.basename(image_path)}\")\n",
        "    plt.show()\n",
        "\n",
        "    base_name = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(base_name)\n",
        "    annotated_image_path = os.path.join(output_dir_img, f\"{name}_annotated{ext}\")\n",
        "    try:\n",
        "        annotated_img_pil.save(annotated_image_path)\n",
        "        print(f\"\\n‚úÖ Annotated image saved to {annotated_image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving annotated image {annotated_image_path}: {e}\")\n",
        "\n",
        "def process_image_input(input_path, od_model_inf_callable, current_od_model_type_str,\n",
        "                        crnn_model_inf, crnn_img_transform_inf, device_to_use, output_dir_imgs_base):\n",
        "    \"\"\"\n",
        "    Processes a single image or all images in a folder with a progress bar for folder processing.\n",
        "    \"\"\"\n",
        "    if os.path.isfile(input_path):\n",
        "        print(f\"\\n--- Processing single image: {os.path.basename(input_path)} ---\")\n",
        "        process_single_image(input_path, od_model_inf_callable, current_od_model_type_str,\n",
        "                             crnn_model_inf, crnn_img_transform_inf, device_to_use, output_dir_imgs_base)\n",
        "    elif os.path.isdir(input_path):\n",
        "        print(f\"\\n--- Processing images in folder: {input_path} ---\")\n",
        "        image_files = []\n",
        "        # Common image extensions\n",
        "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\", \"*.webp\"]:\n",
        "            image_files.extend(glob.glob(os.path.join(input_path, ext.lower())))\n",
        "            image_files.extend(glob.glob(os.path.join(input_path, ext.upper()))) # Handle uppercase extensions\n",
        "        image_files = sorted(list(set(image_files))) # Remove duplicates and sort\n",
        "\n",
        "        if not image_files:\n",
        "            print(f\"‚ùå No images found in {input_path}\")\n",
        "            return\n",
        "\n",
        "        # Use tqdm for the loop over image files\n",
        "        for image_file in tqdm(image_files, desc=\"Processing images in folder\", unit=\"image\"):\n",
        "            print(f\"\\n--- Processing {os.path.basename(image_file)} ---\") # Still useful to know which file is next\n",
        "            process_single_image(image_file, od_model_inf_callable, current_od_model_type_str,\n",
        "                                 crnn_model_inf, crnn_img_transform_inf, device_to_use, output_dir_imgs_base)\n",
        "        print(\"\\n‚úÖ Finished processing all images in the folder.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Input path {input_path} is not a valid file or directory.\")\n",
        "\n",
        "print(\"‚úÖ Image processing functions defined\")"
      ],
      "metadata": {
        "id": "biAceYv1EQVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Processing"
      ],
      "metadata": {
        "id": "ZhdLAEokEsOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_video(\n",
        "    video_path, od_model_inf_callable, current_od_model_type_str, crnn_model_inf, crnn_img_transform_inf, device_to_use, output_dir_vid,\n",
        "    display_frames=True, display_interval=1,\n",
        "    iou_threshold=0.3, smoothing_window_size=7,\n",
        "    max_track_age=10,\n",
        "    plausible_plate_threshold=0.3,\n",
        "    hysteresis_count=2\n",
        "):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ùå Error opening video file {video_path}\")\n",
        "        return\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    base_name = os.path.basename(video_path)\n",
        "    name, ext = os.path.splitext(base_name)\n",
        "    annotated_video_path = os.path.join(output_dir_vid, f\"{name}_annotated.mp4\")\n",
        "    out = cv2.VideoWriter(annotated_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    print(f\"Processing video: {video_path} ({total_frames} frames)\")\n",
        "    print(f\"Parameters: IoU={iou_threshold}, Age={max_track_age}, Window={smoothing_window_size}, PlausConf={plausible_plate_threshold}, Hyst={hysteresis_count}\")\n",
        "    print(f\"Output will be saved to: {annotated_video_path}\")\n",
        "\n",
        "    font_cv = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.6\n",
        "    box_color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "    text_bg_color = (0, 255, 0)\n",
        "    text_font_color = (0,0,0)\n",
        "\n",
        "    active_tracks = {}\n",
        "    next_track_id = 0\n",
        "    processed_frames_count_for_log = 0\n",
        "\n",
        "    # For displaying frames\n",
        "    if display_frames:\n",
        "         from IPython.display import display, clear_output\n",
        "         import ipywidgets as widgets\n",
        "         from IPython.display import HTML, display as ipydisplay\n",
        "         from base64 import b64encode\n",
        "\n",
        "         # Setup for displaying video in Colab output\n",
        "         output_widget = widgets.Output()\n",
        "         ipydisplay(output_widget)\n",
        "\n",
        "    for frame_count_idx in tqdm(range(total_frames), desc=f\"Processing {base_name}\", unit=\"frame\"):\n",
        "        ret, frame_cv = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"\\n‚ö†Ô∏è Warning: Could not read frame {frame_count_idx + 1}/{total_frames}. Ending video processing early\")\n",
        "            break\n",
        "        processed_frames_count_for_log = frame_count_idx + 1\n",
        "\n",
        "        frame_pil = Image.fromarray(cv2.cvtColor(frame_cv, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # --- Object Detection Call ---\n",
        "        od_results = None\n",
        "        if current_od_model_type_str.startswith(\"yolo\"):\n",
        "            od_results = od_model_inf_callable(frame_pil, verbose=False, conf=0.25)\n",
        "        elif current_od_model_type_str == \"efficientdet\":\n",
        "            od_results = run_efficientdet_inference(\n",
        "                tf_infer_function=od_model_inf_callable,\n",
        "                pil_image=frame_pil,\n",
        "                expected_input_size=(512, 512),\n",
        "                score_threshold=0.35,\n",
        "                license_plate_class_id=1 # VERIFY THIS\n",
        "            )\n",
        "        else:\n",
        "            print(f\"‚ùå Unknown OD model type for inference in video: {current_od_model_type_str}\")\n",
        "            out.write(annotated_frame_cv) # Write unannotated frame\n",
        "            continue\n",
        "        # --- End Object Detection Call ---\n",
        "\n",
        "        annotated_frame_cv = frame_cv.copy()\n",
        "\n",
        "        current_frame_detections = [] # Store detections for this frame WITH their assigned track IDs\n",
        "        if od_results and len(od_results[0].boxes) > 0:\n",
        "            for i, box_data in enumerate(od_results[0].boxes):\n",
        "                xyxy = box_data.xyxy[0].cpu().numpy().astype(int)\n",
        "                od_conf = box_data.conf[0].cpu().numpy()\n",
        "                xmin, ymin, xmax, ymax = xyxy\n",
        "                xmin = max(0, xmin); ymin = max(0, ymin)\n",
        "                xmax = min(frame_pil.width, xmax); ymax = min(frame_pil.height, ymax)\n",
        "                if xmax <= xmin or ymax <= ymin: continue\n",
        "\n",
        "                cropped_plate_pil = frame_pil.crop((xmin, ymin, xmax, ymax))\n",
        "                raw_text, raw_conf = \"N/A\", 0.0\n",
        "                if cropped_plate_pil.width > 5 and cropped_plate_pil.height > 5:\n",
        "                    raw_text, raw_conf = predict_plate_with_crnn_and_conf(cropped_plate_pil, crnn_model_inf, crnn_img_transform_inf, device_to_use)\n",
        "\n",
        "                current_frame_detections.append({\n",
        "                    'bbox': (xmin, ymin, xmax, ymax), 'raw_text': raw_text,\n",
        "                    'raw_conf': raw_conf, 'od_conf': od_conf,\n",
        "                    'assigned_track_id': None # Will be filled if matched\n",
        "                })\n",
        "\n",
        "        # --- Track Management ---\n",
        "        matched_track_ids_this_frame = set()\n",
        "\n",
        "        for det_idx, det_data in enumerate(current_frame_detections): # Iterate over copy for modification\n",
        "            best_iou_val = 0; matched_id = -1\n",
        "            for track_id_key, track_data_val in active_tracks.items():\n",
        "                iou_val = calculate_iou(det_data['bbox'], track_data_val['last_bbox'])\n",
        "                if iou_val > iou_threshold and iou_val > best_iou_val:\n",
        "                    best_iou_val = iou_val\n",
        "                    matched_id = track_id_key\n",
        "\n",
        "            if matched_id != -1:\n",
        "                track = active_tracks[matched_id]\n",
        "                track['last_bbox'] = det_data['bbox'] # CRITICAL: Update bbox with current detection\n",
        "                track['age'] = 0\n",
        "                track['visible_this_frame'] = True\n",
        "                current_frame_detections[det_idx]['assigned_track_id'] = matched_id # Store track_id with current detection\n",
        "                matched_track_ids_this_frame.add(matched_id)\n",
        "\n",
        "                is_plausible = is_plausible_malaysian_plate(det_data['raw_text'])\n",
        "                if is_plausible and det_data['raw_conf'] >= plausible_plate_threshold:\n",
        "                    track['text_conf_history'].append((det_data['raw_text'], det_data['raw_conf']))\n",
        "                elif is_plausible:\n",
        "                    track['text_conf_history'].append((det_data['raw_text'], det_data['raw_conf'])) # Store even if low conf but plausible\n",
        "\n",
        "                while len(track['text_conf_history']) > smoothing_window_size:\n",
        "                    track['text_conf_history'].pop(0)\n",
        "\n",
        "                if track['text_conf_history']:\n",
        "                    texts_in_history = [item[0] for item in track['text_conf_history']]\n",
        "                    vote_counts = Counter(texts_in_history)\n",
        "                    if vote_counts:\n",
        "                        most_common_text, count = vote_counts.most_common(1)[0]\n",
        "                        if most_common_text != track['smoothed_text']:\n",
        "                            if track['candidate_text'] == most_common_text:\n",
        "                                track['candidate_persistence'] +=1\n",
        "                            else:\n",
        "                                track['candidate_text'] = most_common_text\n",
        "                                track['candidate_persistence'] = 1\n",
        "                            if track['candidate_persistence'] >= hysteresis_count:\n",
        "                                track['smoothed_text'] = most_common_text\n",
        "                                confs_for_smoothed_text = [item[1] for item in track['text_conf_history'] if item[0] == most_common_text]\n",
        "                                track['smoothed_conf'] = np.mean(confs_for_smoothed_text) if confs_for_smoothed_text else 0.0\n",
        "                                track['candidate_persistence'] = 0\n",
        "                        else:\n",
        "                            track['candidate_persistence'] = 0\n",
        "                            confs_for_smoothed_text = [item[1] for item in track['text_conf_history'] if item[0] == track['smoothed_text']]\n",
        "                            track['smoothed_conf'] = np.mean(confs_for_smoothed_text) if confs_for_smoothed_text else track['smoothed_conf']\n",
        "            # else:\n",
        "                # This detection is unmatched, will be considered for a new track later if not assigned.\n",
        "\n",
        "\n",
        "        ids_to_remove = []\n",
        "        for track_id_key, track_data_val in active_tracks.items():\n",
        "            if not track_data_val.get('visible_this_frame', False):\n",
        "                track_data_val['age'] += 1\n",
        "            if track_data_val['age'] > max_track_age:\n",
        "                ids_to_remove.append(track_id_key)\n",
        "            track_data_val['visible_this_frame'] = False # Reset for next frame\n",
        "        for r_id in ids_to_remove:\n",
        "            if r_id in active_tracks:\n",
        "                del active_tracks[r_id]\n",
        "\n",
        "        for det_idx, det_data in enumerate(current_frame_detections): # Iterate again to create new tracks for truly unmatched\n",
        "            if det_data['assigned_track_id'] is None: # Still unmatched after trying to associate with existing tracks\n",
        "                is_plausible = is_plausible_malaysian_plate(det_data['raw_text'])\n",
        "                initial_text_conf_history = []\n",
        "                if is_plausible and det_data['raw_conf'] >= plausible_plate_threshold:\n",
        "                     initial_text_conf_history.append((det_data['raw_text'], det_data['raw_conf']))\n",
        "\n",
        "                active_tracks[next_track_id] = {\n",
        "                    'last_bbox': det_data['bbox'], # Current bbox\n",
        "                    'text_conf_history': initial_text_conf_history,\n",
        "                    'smoothed_text': det_data['raw_text'] if is_plausible else \"N/A\",\n",
        "                    'smoothed_conf': det_data['raw_conf'] if is_plausible else 0.0,\n",
        "                    'age': 0, 'visible_this_frame': True,\n",
        "                    'candidate_text': None, 'candidate_persistence': 0\n",
        "                }\n",
        "                current_frame_detections[det_idx]['assigned_track_id'] = next_track_id # Assign new track_id to current detection\n",
        "                next_track_id += 1\n",
        "\n",
        "        # Iterate through current_frame_detections. If a detection was assigned a track,\n",
        "        # use its current bbox and the track's smoothed text.\n",
        "        for det_data in current_frame_detections:\n",
        "            if det_data['assigned_track_id'] is not None:\n",
        "                track_id_key = det_data['assigned_track_id']\n",
        "                # Ensure the track still exists (it should, as we just updated/created it)\n",
        "                if track_id_key in active_tracks:\n",
        "                    track_data_val = active_tracks[track_id_key]\n",
        "\n",
        "                    # Use the CURRENT detection's bbox for drawing\n",
        "                    xmin, ymin, xmax, ymax = det_data['bbox']\n",
        "\n",
        "                    # Use the track's smoothed text and confidence\n",
        "                    display_text = track_data_val['smoothed_text']\n",
        "                    display_crnn_conf = track_data_val['smoothed_conf']\n",
        "\n",
        "                    # OD confidence from the current detection\n",
        "                    display_od_conf = det_data['od_conf']\n",
        "\n",
        "                    label = f\"{display_text} (C:{display_crnn_conf:.2f} | OD:{display_od_conf:.2f})\"\n",
        "\n",
        "                    cv2.rectangle(annotated_frame_cv, (xmin, ymin), (xmax, ymax), box_color, thickness)\n",
        "                    (txt_w, txt_h), base = cv2.getTextSize(label, font_cv, font_scale, thickness)\n",
        "                    txt_y_coord = ymin - txt_h - base - 5\n",
        "                    bg_y1_coord = txt_y_coord - base; bg_y2_coord = ymin - base + 5\n",
        "                    if bg_y1_coord < 0:\n",
        "                        txt_y_coord = ymax + txt_h + 5\n",
        "                        bg_y1_coord = ymax+5\n",
        "                        bg_y2_coord = ymax+txt_h+base+5\n",
        "                    cv2.rectangle(annotated_frame_cv, (xmin, bg_y1_coord), (xmin + txt_w + 5, bg_y2_coord), text_bg_color, -1)\n",
        "                    cv2.putText(annotated_frame_cv, label, (xmin + 2, txt_y_coord + txt_h), font_cv, font_scale, text_font_color, thickness, cv2.LINE_AA)\n",
        "\n",
        "        out.write(annotated_frame_cv)\n",
        "        if display_frames and (frame_count_idx + 1) % display_interval == 0:\n",
        "            print(f\"\\nDisplaying frame {frame_count_idx + 1}/{total_frames} of {base_name}\")\n",
        "            display_img = cv2.cvtColor(annotated_frame_cv, cv2.COLOR_BGR2RGB)\n",
        "            display(IPImage(data=cv2.imencode('.jpeg', annotated_frame_cv)[1].tobytes()))\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"\\n‚úÖ Video processing complete for {base_name}. {processed_frames_count_for_log} frames processed\")\n",
        "    print(f\"‚úÖ Annotated video saved to {annotated_video_path}\")\n",
        "\n",
        "def process_video_input(input_path, od_model, current_od_model_name, crnn_model, crnn_image_transform, device, output_dir,\n",
        "                        display_frames=True, display_interval=15,\n",
        "                        iou_threshold=0.3, smoothing_window_size=5, max_track_age=10,\n",
        "                        plausible_plate_threshold=0.3, hysteresis_count=2):\n",
        "    \"\"\"\n",
        "    Processes a single video or all videos in a folder.\n",
        "    Selected frames are displayed sequentially.\n",
        "    \"\"\"\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.mpg', '.mpeg', '.ts']\n",
        "    if os.path.isfile(input_path):\n",
        "        if any(input_path.lower().endswith(ext) for ext in video_extensions):\n",
        "            print(f\"\\n--- Processing single video: {os.path.basename(input_path)} ---\")\n",
        "            process_single_video(input_path, od_model, current_od_model_name, crnn_model, crnn_image_transform, device, output_dir,\n",
        "                         display_frames, display_interval,\n",
        "                         iou_threshold, smoothing_window_size, max_track_age,\n",
        "                         plausible_plate_threshold, hysteresis_count)\n",
        "        else:\n",
        "            print(f\"‚ùå {input_path} is not a recognized video file type (checked: {', '.join(video_extensions)}).\")\n",
        "    elif os.path.isdir(input_path):\n",
        "        print(f\"\\n--- Processing videos in folder: {input_path} ---\")\n",
        "        video_files = []\n",
        "        for ext_pattern in [\"*.mp4\", \"*.avi\", \"*.mov\", \"*.mkv\", \"*.wmv\", \"*.flv\", \"*.webm\", \"*.mpg\", \"*.mpeg\", \"*.ts\"]:\n",
        "            video_files.extend(glob.glob(os.path.join(input_path, ext_pattern.lower())))\n",
        "            video_files.extend(glob.glob(os.path.join(input_path, ext_pattern.upper())))\n",
        "        video_files = sorted(list(set(video_files)))\n",
        "\n",
        "        if not video_files:\n",
        "            print(f\"‚ùå No videos found in {input_path}\")\n",
        "            return\n",
        "\n",
        "        for video_file in tqdm(video_files, desc=\"Processing video folder\", unit=\"video\"):\n",
        "            print(f\"\\n--- Processing video: {os.path.basename(video_file)} ---\")\n",
        "            process_single_video(video_file, od_model, current_od_model_name, crnn_model, crnn_image_transform, device, output_dir,\n",
        "                         display_frames, display_interval,\n",
        "                         iou_threshold, smoothing_window_size, max_track_age,\n",
        "                         plausible_plate_threshold, hysteresis_count)\n",
        "        print(\"\\n‚úÖ Finished processing all videos in the folder.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Input path {input_path} is not a valid file or directory.\")\n",
        "\n",
        "print(\"‚úÖ Video processing functions defined\")"
      ],
      "metadata": {
        "id": "lOOVR6rPEy3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models and Run Processing"
      ],
      "metadata": {
        "id": "nySv-Xf1E7is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Choose which OD model to use ---\n",
        "# Option 1: YOLOv11\n",
        "od_model_to_use_path = yolov11_model_path\n",
        "current_od_model_name = \"yolov11\"\n",
        "\n",
        "# Option 2: YOLOv10\n",
        "# od_model_to_use_path = yolov10_model_path\n",
        "# current_od_model_name = \"yolov10\"\n",
        "\n",
        "# Option 3: EfficientDet\n",
        "# od_model_to_use_path = efficientdet_model_path\n",
        "# current_od_model_name = \"efficientdet\"\n",
        "\n",
        "# --- Load the selected OD model ---\n",
        "object_detector_callable = None\n",
        "if not os.path.exists(od_model_to_use_path):\n",
        "    print(f\"‚ùå FATAL: Chosen OD model path does not exist: {od_model_to_use_path}\")\n",
        "else:\n",
        "    print(f\"Attempting to load OD model '{current_od_model_name}' from: {od_model_to_use_path}\")\n",
        "    if current_od_model_name.startswith(\"yolo\"):\n",
        "        object_detector_callable = load_od_model(od_model_to_use_path, device)\n",
        "    elif current_od_model_name == \"efficientdet\":\n",
        "        object_detector_callable = load_tf_saved_model(od_model_to_use_path)\n",
        "    else:\n",
        "        print(f\"‚ùå Unknown OD model type specified: {current_od_model_name}\")\n",
        "\n",
        "img_height = 32 # CRNN input height\n",
        "\n",
        "# --- Load CRNN model ---\n",
        "crnn_recognizer = None\n",
        "if not os.path.exists(best_model_path):\n",
        "    print(f\"‚ùå FATAL: CRNN Model weights not found: {best_model_path}\")\n",
        "else:\n",
        "    print(f\"\\nUsing CRNN model from: {best_model_path}\")\n",
        "    crnn_recognizer = load_crnn_for_inference(best_model_path, img_height, num_classes, device)"
      ],
      "metadata": {
        "id": "-d_r1xA1Qnm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if object_detector_callable and crnn_recognizer:\n",
        "    print(\"‚úÖ All models loaded successfully for pipeline.\")\n",
        "\n",
        "    # Determine output paths based on the OD model being used\n",
        "    if current_od_model_name == \"yolov11\":\n",
        "        model_specific_output_base = os.path.join(annotated_output_path, 'yolov11')\n",
        "        print(f\"Outputting results to YOLOv11 specific directory: {model_specific_output_base}\")\n",
        "    elif current_od_model_name == \"yolov10\":\n",
        "        model_specific_output_base = os.path.join(annotated_output_path, 'yolov10')\n",
        "        print(f\"Outputting results to YOLOv10 specific directory: {model_specific_output_base}\")\n",
        "    elif current_od_model_name == \"efficientdet\":\n",
        "        model_specific_output_base = os.path.join(annotated_output_path, 'efficientdet')\n",
        "        print(f\"Outputting results to EfficientDet specific directory: {model_specific_output_base}\")\n",
        "    else:\n",
        "        # Fallback for any other model name or if current_od_model_name is not set properly\n",
        "        model_specific_output_base = os.path.join(annotated_output_path, 'unknown_od_model')\n",
        "        print(f\"‚ö†Ô∏è Warning: Unknown OD model name '{current_od_model_name}'. Outputting to: {model_specific_output_base}\")\n",
        "\n",
        "    # Create model-specific subdirectories for images and videos\n",
        "    pipeline_output_images_path = os.path.join(model_specific_output_base, 'images')\n",
        "    pipeline_output_videos_path = os.path.join(model_specific_output_base, 'videos')\n",
        "    os.makedirs(model_specific_output_base, exist_ok=True)\n",
        "    os.makedirs(pipeline_output_images_path, exist_ok=True)\n",
        "    os.makedirs(pipeline_output_videos_path, exist_ok=True)\n",
        "\n",
        "    # --- Test Cases (using the determined pipeline_output_images and pipeline_output_videos) ---\n",
        "\n",
        "    # Test with a single image\n",
        "    # sample_image_path = \"/content/gdrive/MyDrive/cos30018/NO20250513-132446-130571F_frame_1500.jpg\"\n",
        "    # if os.path.exists(sample_image_path):\n",
        "    #    process_image_input(sample_image_path, object_detector_callable, current_od_model_name, crnn_recognizer, crnn_inference_transform, device, pipeline_output_images_path)\n",
        "    # else:\n",
        "    #    print(f\"‚ùå Image {sample_image_path} not found. Skipping.\")\n",
        "\n",
        "    # Test with an image folder\n",
        "    sample_image_folder = \"/content/gdrive/MyDrive/cos30018/test_image\"\n",
        "    if os.path.exists(sample_image_folder) and os.listdir(sample_image_folder):\n",
        "         process_image_input(sample_image_folder, object_detector_callable, current_od_model_name, crnn_recognizer, crnn_inference_transform, device, pipeline_output_images_path)\n",
        "    else:\n",
        "        print(f\"‚ùå Image folder {sample_image_folder} not found or is empty. Skipping.\")\n",
        "\n",
        "    # Test with a single video\n",
        "    sample_video_path = \"/content/gdrive/MyDrive/cos30018/20250426070025_041707.TS\"\n",
        "    if os.path.exists(sample_video_path):\n",
        "        process_video_input(\n",
        "            sample_video_path,\n",
        "            object_detector_callable,\n",
        "            current_od_model_name,\n",
        "            crnn_recognizer,\n",
        "            crnn_inference_transform,\n",
        "            device,\n",
        "            output_dir=pipeline_output_videos_path, # Changed from positional to keyword arg\n",
        "            display_frames=True,\n",
        "            display_interval=30,\n",
        "            iou_threshold=0.3,\n",
        "            smoothing_window_size=10,\n",
        "            max_track_age=12,\n",
        "            plausible_plate_threshold=0.25,\n",
        "            hysteresis_count=4\n",
        "        )\n",
        "    else:\n",
        "        print(f\"‚ùå Video {sample_video_path} not found. Skipping.\")\n",
        "\n",
        "    # Test with a video folder\n",
        "    # sample_video_folder = \"/content/gdrive/MyDrive/cos30018/test_video\"\n",
        "    # video_extensions_check = ('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.mpg', '.mpeg', '.ts')\n",
        "    # if os.path.exists(sample_video_folder) and any(f.lower().endswith(video_extensions_check) for f in os.listdir(sample_video_folder)):\n",
        "    #     process_video_input(\n",
        "    #         sample_video_folder,\n",
        "    #         object_detector_callable,\n",
        "    #         current_od_model_name,\n",
        "    #         crnn_recognizer,\n",
        "    #         crnn_inference_transform,\n",
        "    #         device,\n",
        "    #         output_dir=pipeline_output_videos_path, # Changed from positional to keyword arg\n",
        "    #         display_frames=True,\n",
        "    #         display_interval=30,\n",
        "    #         iou_threshold=0.3,\n",
        "    #         smoothing_window_size=10,\n",
        "    #         max_track_age=12,\n",
        "    #         plausible_plate_threshold=0.25,\n",
        "    #         hysteresis_count=4\n",
        "    #     )\n",
        "    # else:\n",
        "    #     print(f\"‚ùå Video folder {sample_video_folder} not found or is empty or contains no videos. Skipping.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Pipeline processing finished for OD model:\", current_od_model_name)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå One or more models failed to load. Cannot proceed with processing\")\n",
        "    if not object_detector_callable:\n",
        "        print(f\"‚ùå Object Detector failed to load. Check path related to '{current_od_model_name if 'current_od_model_name' in locals() else 'chosen OD model'}'.\")\n",
        "    if not crnn_recognizer:\n",
        "        print(f\"‚ùå CRNN Recognizer failed to load. Check path: {best_model_path}\")"
      ],
      "metadata": {
        "id": "L2mqEXVhQ7Vw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}